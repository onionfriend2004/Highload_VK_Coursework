# Highload VK Education

[Задание](https://github.com/init/highload/blob/main/homework_architecture.md)

Курсовая работа по дисциплине "Проектирование высоконагруженных систем"

*Лупенков Алексей, осень 2025*

## Содержание
* [**1. Тема, аудитория, функционал**](#1-тема-аудитория-функционал)

* [**2. Расчёт нагрузки**](#2-расчёт-нагрузки)

* [**3. Глобальная балансировка нагрузки**](#3-глобальная-балансировка-нагрузки)

* [**4. Локальная балансировка нагрузки**](#4-локальная-балансировка-нагрузки)

* [**5. Логическая схема БД**](#5-логическая-схема-бд)

* [**6. Физическая схема БД**](#6-физическая-схема-бд)

* [**7. Алгоритмы**](#7-алгоритмы)

* [**8. Технологии**](#8-технологии)

* [**9. Обеспечение надежности**](#9-обеспечение-надежности)

* [**10. Схема проекта**](#10-схема-проекта)

## 1. Тема, аудитория, функционал

### Тема

Zoom - сервис для видеоконференций

### Аудитория

#### Мировой рынок [[1](https://www.demandsage.com/zoom-statistics/)]
* Пользователи:
  * ```700 млн. MAU```[[2](https://www.marketingscoop.com/blog/zoom-monthly-active-users-the-ultimate-guide-2024/)]
  * ```300 млн. DAU```
  * ```3.3 трлн. минут встреч в год```
  * ```45 млрд. записанных минут встреч в год```
  * ```среднее количество участников 10 человек```[[4](https://www.zoom.com/en/blog/how-you-zoomed-over-the-past-year-2021/?lang=en-US)]
  * ```Средняя продолжительность конференции 54 минуты```
  * ```Для видео формата HD пропускная способность должна быть 1.8 МБит/с на загрузку``` [[5](https://support.zoom.com/hc/ru/article?id=zm_kb&sysparm_article=KB0060759)]

#### Распределение по странам [[3](https://www.statista.com/statistics/1259936/distribution-of-zoomus-traffic-by-country/)]
* *США - ```42.06%``` пользователей*
* *Япония - ```4.26%``` пользователей*
* *Канада - ```4.14%``` пользователей*
* *Индия - ```3.86%``` пользователей*
* *Великобритания - ```3.86%``` пользователей*
* *Остальные страны - ```41.82%``` пользователей*

### Функционал

Ключевой функционал - аудио/видео связь

Ключевое продуктовое решение - запись звонка на стороне сервера, вход по ссылке

* Регистрация и авторизация
* Создание конференции
    * Аудио/видео связь
    * Запись звонка
    * Удаление/добавление участников
* Вход в конференцию по ссылке
* Текстовый чат

## 2. Расчёт нагрузки

* Допущения:
    * Размер аватарки за 400×400 пикселей в формате jpeg
    * Имя пользователя максимум может содержать 20 символов UTF-8
    * Информация о пользователе может содержать максимум 500 символов UTF-8.
    * 1 час видео в формате HD занимает примерно 1 Гб
    * Пользователь входит  раз в месяц
    * 1 пользователь отправляет 0.5 сообщений за конференцию
    * В 1 конференции чат содержит в среднем 150 символов
    * Пиковая нагрузка в 3 раза больше средней

### Расчет среднего размера хранилища на одного пользователя
* Фото польователя и персональная информация

$$100 + \frac{520}{1024} \approx 101 \space Кб$$

* Записи конференций

$$\frac{3.3 \cdot 10^{12}}{54 \cdot 10 \cdot 365} \approx 16742770 \space встреч \space в  \space день$$

Соотношение записанных минут встреч к незаписанным:

$$\frac{45 \cdot 10^{9}}{3.3 \cdot 10^{12}} \approx 0.0136$$

Количество записанных встреч в день:

$$16742770 \cdot 0.0136 \approx 227701 \space записанных \space встреч \space в  \space день$$

Количество записей в месяц на 1 пользователя:

$$\frac{227701}{7 \cdot 10^8} \cdot 30 \approx 0.0098$$

* Планирование конференций, информация о ссылках и т.п.

Содержит 500 символов UTF-8:

$$\frac{500}{1024} \approx 0.5 \space Кб$$

### Расчет среднего количества действий пользователя по типам в день
* Авторизация

Пусть пользователь входит  раз в месяц

* Создание конференции

$$\frac{16742770}{7 \cdot 10^{8}} \cdot 30 \approx 0.7 \space конференций \space в \space месяц$$

* Отправка сообщений в чат

Пусть 1 пользователь отправляет 0.5 сообщений за конференцию, тогда:

$$0.5 \cdot 7 \approx 4.5 \space сообщений \space в \space месяц$$

* Присоединение к конференции

$$0.7 \cdot 10 \approx 7 \space присоединений \space в \space месяц$$

* Сохранение записей

Пользователь сохраняет 0.01 встречу в месяц

### Продуктовые метрики

| Метрика                                | Значение                  |
|----------------------------------------|---------------------------|
| Месячная аудитория (MAU)               | 700 млн пользователей     |
| Дневная аудитория (DAU)                | 300 млн пользователей     |
| Профиль (аватар + данные)              | ~101 КБ                   |
| Планирование конференций               | ~0.5 КБ                   |
| Записи конференций                     | ~0.01 ГБ                  |
| Авторизация                            | 0.03/день                 |
| Создание конференций                   | 0.023/день                |
| Присоединение к конференциям           | 0.23/день                 |
| Отправка сообщений в чат               | 0.15/день                 |
| Сохранение записей                     | 0.0003/день               |

### Расчет размера хранилища данных

* Фото и персональная информация

$$\frac{101 \cdot 7 \cdot 10^8}{1024 \cdot 1024 \cdot 1024} = 67.43  \space Тб/мес$$

* Записи конференций

$$\frac{0.01 \cdot 7 \cdot 10^8}{1024\cdot 1024} = 6.68 \space Пб$$

* Планирование конференций, информация о ссылках и т.п.

$$\frac{0.5 \cdot 7 \cdot 10^8}{1024 \cdot 1024} = 333 \space Мб/мес$$

* Записи чатов

Пусть в 1 конференции чат содержит в среднем 150 символов, тогда:

$$\frac{150 \cdot 4 \cdot 7 \cdot 10^8}{1024^4} = 0.38 \space Тб$$

* Хранилище сессий

Одна сессия (sessionID, userID):
$$16 + 8 \space Б = 24 \space Б$$

$$\frac{24 \cdot 7 \cdot 10^8}{1024 \cdot 1024 \cdot 1024} = 15.65 \space Гб/мес$$

* Хранилище звонков

Один звонок (id, таймстемпы начала и конца, userIDs) байта:

$$16 + 16 + 16 + 8 \cdot 10 = 128 \space Б$$ 

$$\frac{128 \cdot 7 \cdot 10^8}{1024 \cdot 1024 \cdot 1024} = 83.45 \space Гб/мес$$ 

### Расчет сетевого трафика

* Пропускная способность видео на пользователя

$$1.8 \space Мбит/с  =  0.0018 \space Гбит/с$$ 

* Пропускная способность видео для всех пользователей

$$ \frac{16742770 \cdot 8}{86400} =  1 550.26 \space Гбит/с = 1.6 \space Тбит/с$$ 

* Передача аудио на пользователя

$$10^{-4} \space Гбит/с$$

* Передача аудио для всех пользователей

$$\frac{0.075 \cdot 16742770 \cdot 8}{86400} = 116 \space Гбит/с$$ 

* Сохранение записи в облаке для всех пользователей

$$\frac{227701 \cdot 8}{86400} =  21 \space Гбит/с$$ 

| Тип трафика                          | Базовый     | Пиковый        | Суммарно в сутки  |
|--------------------------------------|----------------------|-------------------------|-----------------------------|
| Пропускная способность видео для всех пользователей |1.6 Тбит/с| 4.8 Тбит/с| 16 Пб/сутки|
| Передача аудио для всех пользователей            |116 Гбит/с| 348 Гбит/с| 1.2 Пб/сутки|
| Сохранение записи в облаке            |21 Гбит/с|63 Гбит/с| 0,23 Пб/сутки|

### RPS 
| Действие                               | RPS в пике                | RPS                     |
|----------------------------------------|---------------------------|-------------------------|
| Авторизация                            | 405     |$$\frac{0.5 \cdot 7 \cdot 10^8}{30 \cdot 24 \cdot 3600} =  135$$ |
| Создание конференции                   | 567     |$$\frac{0.7 \cdot 7 \cdot 10^8}{30 \cdot 24 \cdot 3600} =  189$$ |
| Присоединение к конференции            | 5670     |$$\frac{7 \cdot 7 \cdot 10^8}{30 \cdot 24 \cdot 3600} =  1890$$ |
| Отправка сообщений в чат               | 405     |$$\frac{0.5 \cdot 7 \cdot 10^8}{30 \cdot 24 \cdot 3600} =  135$$ |
| Сохранение записи.                     | 9     |$$\frac{0.01 \cdot 7 \cdot 10^8}{30 \cdot 24 \cdot 3600} =  3$$ |

## 3. Глобальная балансировка нагрузки

### Функциональные группы

#### 1. управляющие 

* Обеспечивают авторизацию и аутентификацию пользователей  
* Хранят и обрабатывают метаданные (профили, настройки, расписания встреч)  
* Управляют сигналингом (установление и завершение сессий, распределение ролей — хост, ко-хост, участники) 

#### 2. медиа хосты

* Обеспечивают передачу аудио и видео в реальном времени  
* Выполняют микширование потоков (например, аудио) или форвардинг видео-потоков  
* Поддерживают механизмы адаптации качества (адаптивный битрейт, переключение разрешения)  

### Расположение датацентров

Согласно [распределению пользователей](#аудитория) по странам и карте девайсов подключенным к интернету, выберем расположение датацентров.

![internet_usage](pics/internet_usage.png)

В США и Канаде наибольшая доля пользователей Zoom, поэтому расположим датацентры на Западном, Восточном побережьях и в центре США.

* Сан-Франциско (медиа хост)
* Нью-Йорк (медиа хост)
* Канзас-Сити (управляющий)

Европа - второй по числу пользователей регион: 

* Лондон (управляющий, медиа хост)
* Франкфурт (медиа хост)

Расположим ЦОД в Японии для покрытия юго-восточной Азии и Дальнего Востока РФ:

* Токио (управляющий, медиа хост)

Для западной Азии и Индии расположим ДЦ в этом регионе:

* Мумбаи (управляющий, медиа хост)

Сибирь и часть Азии будет обслуживать датацентр в Казахстане:
* Астана (медиа хост)

Поскольку Африка является самым непопулярным регионом, то для континента хватит одного ЦОДа на юге:

* Найроби (медиа хост)

Южную Америку будет обслуживать датацентр в Рио-де-Жанейро:

* Рио-де-Жанейро (медиа хост)

Для Австралии и Океании также будет будет достаточно одного датацентра в крупнейшем городе:

* Сидней (управляющий, медиа хост)

Также расположим дополнительный ДЦ между Азией и Австралией:

* Сингапур (медиа хост)

[Интерактивная карта](https://yandex.ru/maps/?um=constructor%3Afd74a62d0a1464f52aa56500eae09796b20fda56d0a5e416cebaf46e40ee8e8b&source=constructorLink)

| Дата-центр     | Авторизация  | Создание конференции | Сообщение в чате | Присоединение | Сохранение записи |
| -------------- | ------------ | -------------------- | ---------------- | ------------- | ----------------- |
| Сан-Франциско  | 0 / 0        | 0 / 0                | 0 / 0            | 282.0 / 883.4 | 0.46 / 1.38       |
| Нью-Йорк       | 0 / 0        | 0 / 0                | 0 / 0            | 282.0 / 883.4 | 0.46 / 1.38       |
| Канзас-Сити    | 66.6 / 199.6 | 93.6 / 279.0         | 66.6 / 199.6     | 0 / 0         | 0 / 0             |
| Лондон         | 16.7 / 50.0  | 23.3 / 70.1          | 16.7 / 50.0      | 70.7 / 221.5  | 0.12 / 0.35       |
| Франкфурт      | 0 / 0        | 0 / 0                | 0 / 0            | 255.3 / 798.5 | 0.42 / 1.24       |
| Токио          | 18.4 / 55.2  | 25.8 / 77.3          | 18.4 / 55.2      | 78.0 / 244.2  | 0.13 / 0.38       |
| Мумбаи         | 16.7 / 50.0  | 23.3 / 70.1          | 16.7 / 50.0      | 70.7 / 221.5  | 0.12 / 0.35       |
| Астана         | 0 / 0        | 0 / 0                | 0 / 0            | 255.3 / 798.5 | 0.42 / 1.24       |
| Сингапур       | 0 / 0        | 0 / 0                | 0 / 0            | 255.3 / 798.5 | 0.42 / 1.24       |
| Найроби        | 0 / 0        | 0 / 0                | 0 / 0            | 255.3 / 798.5 | 0.42 / 1.24       |
| Рио-де-Жанейро | 0 / 0        | 0 / 0                | 0 / 0            | 255.3 / 798.5 | 0.42 / 1.24       |
| Сидней         | 16.7 / 50.0  | 23.3 / 70.1          | 16.7 / 50.0      | 70.7 / 221.5  | 0.12 / 0.35       |
### Схема глобальной балансировки

Применяется DNS

#### Схема DNS-балансировки
1. Управляющие

```mermaid
flowchart LR
    U[Пользователь] --> D[GeoDNS]
    D -->|по региону / нагрузке| R1[Регион 1]
    D -->|по региону / нагрузке| R2[Регион 2]
    D -->|резерв| R3[Регион 3]

    %% Регион 1
    subgraph "Регион 1"
        R1 --> LB1[L7 Балансировщик]
        LB1 --> P1[БД]
    end

    %% Регион 2
    subgraph "Регион 2"
        R2 --> LB2[L7 Балансировщик]
        LB2 --> P2[БД]
    end

    %% Регион 3 (Failover)
    subgraph "Регион 3 (Failover)"
        R3 --> LB3[L7 Балансировщик]
        LB3 --> P3[БД]
    end
```

2. Медиа хосты

```mermaid
flowchart LR
    U[Пользователь] --> D[GeoDNS]
    D -->|по региону / нагрузке| R1[Регион 1]
    D -->|по региону / нагрузке| R2[Регион 2]
    D -->|резерв| R3[Регион 3]

    %% Регион 1
    subgraph "Регион 1"
        R1 --> In1[ingress]
        In1 --> CP1[Control Plane: Auth / Metadata / Scheduling]
        In1 --> Media1[Media Requests → Media Servers]
    end

    %% Регион 2
    subgraph "Регион 2"
        R2 --> In2[ingress]
        In2 --> CP2[Control Plane: Auth / Metadata / Scheduling]
        In2 --> Media2[Media Requests → Media Servers]
    end

    %% Регион 3 (Failover)
    subgraph "Регион 3 (Failover)"
        R3 --> In3[ingress]
        In3 --> CP3[Control Plane: Auth / Metadata / Scheduling]
        In3 --> Media3[Media Requests → Media Servers]
    end
```


Geo-based DNS — направляет трафик в ближайший регион.

## 4. Локальная балансировка нагрузки

Классические L4/L7-балансировщики не подойдут для stateful видео-конференций, потому что:

- отсутствует интеграция с **room-registry** — готовый LB не знает `room_id → media_host` и может разбросать участников одной комнаты по разным SFU;  
- многие медиапротоколы — **DTLS/SRTP/UDP** — требуют UDP-aware прокси с прозрачным connection tracking, чего обычные L7 LB не обеспечивают;  
- стандартные health-checks (TCP/HTTP) не отражают real-time качество (packet loss, jitter, encoder health), поэтому LB может направлять трафик на деградированные инстансы;  
- готовые LB не поддерживают гибкую политику миграции (переход комнат, graceful reconnect);  

### Схема локальной балансировки нагрузки

```mermaid
flowchart LR
    subgraph CLIENT["Клиент"]
        U["Join Room (HTTPS/WSS)"]
    end

    subgraph EDGE["Edge Layer"]
        I["Ingress Gateway"]
        D["GeoDNS"]
    end

    subgraph CONTROL["Control Plane"]
        C["Room Registry / Load Manager"]
    end

    subgraph MEDIA["Media Layer (SFU)"]
        M["Media Host"]
        H["Service Mesh Health Monitor"]
    end

    %% Signaling path (HTTP/WSS)
    U --> D --> I --> C
    C --> I --> U

    %% Media path (UDP/WebRTC)
    U -. UDP/WebRTC .-> M

    %% Control & health
    M --> C
    H --> C
    C -.-> H
```

1. Клиент подключается к Ingress Gateway регионального дата-центра.  
2. Ingress пересылает запрос в Control Plane этого региона.  
3. Control Plane выбирает оптимальный media-host с учетом параметров загрузки, доступности и привязки комнаты.  
4. Control Plane возвращает Ingress-узлу адрес выбранного media-host.  
5. Ingress возвращает клиенту адрес media-host.  
6. Клиент устанавливает WebRTC/UDP соединение напрямую с media-host.  
7. Media-host подтверждает соединение и регистрирует участника в реестре (room_id → media_host).

### Создание новой комнаты

1. Клиент или сервер отправляет запрос на создание комнаты в Control Plane.  
2. Control Plane выбирает media-host по текущей загрузке и политике размещения, резервирует ресурсы.  
3. Control Plane записывает соответствие `room_id → media_host` в реестр.  
4. Control Plane возвращает участникам адрес выбранного media-host и токен доступа.  
5. Участники устанавливают соединение с media-host через Ingress.

### Присоединение к существующей комнате

1. Клиент отправляет запрос на присоединение (join) в Control Plane с `room_id`.  
2. Control Plane считывает из реестра соответствие `room_id → media_host` и возвращает адрес хоста.  
3. Клиент подключается к media-host; механизм connection tracking удерживает привязку сессии к конкретному хосту.

### Отказ и failover

1. Mesh обнаруживает, что инстанс media-host стал unhealthy.  
2. Control Plane помечает инстанс как недоступный и обновляет реестр.  
3. Для новых подключений Control Plane выбирает альтернативный media-host.  
4. Для активных комнат Control Plane инициирует миграцию — выбирает новый хост, уведомляет участников о reconnect или переводит комнату на резервный инстанс (в зависимости от реализации).  
5. Если весь регион становится недоступным, GeoDNS переключает трафик на резервный регион; клиенты повторно подключаются к новым media-host'ам.

**Формула резервирования оборудования:**

$$
N_\text{резерв} = \frac{N \times 2}{N + 1}
$$

где:

* $N$ — количество основных экземпляров;
* $N_\text{резерв}$ — количество резервных серверов/балансировщиков.

### Расчет количества балансировщиков

$$
\text{BW}_\text{балансировщика} = \text{RPS}_\text{пик} \times \text{размер запроса (ГБ)} \times 8\ (\text{Гбит/с})
$$

#### Результат ingress

Общий пиковый медиатрафик 5211 Gbit/s делим между DC пропорционально присоединениям.

*Сумма всех peak `Присоединение` = 6668 RPS.*

Для каждого DC:
1. Доля = `join_peak_DC / 6668`
2. BW_med_DC = 5211 Gbit/s * доля
3. N_baseline = ceil(BW_med_DC / 80) 
4. N_reserve = ceil((N_baseline * 2) / (N_baseline + 1))
5. N_total = N_baseline + N_reserve

| Дата-центр     | Присоединение (RPS) | baseline (N) | резерв | всего |
|----------------|----------------:|-----------------:|-----------:|---------:|
| Сан-Франциско  | 883.4           |9                | 2          | **11**   |
| Нью-Йорк       | 883.4           | 9                | 2          | **11**   |
| Канзас-Сити    | 0.0             |  0.0                   | 0                | 0          | **0**    |
| Лондон         | 221.5           |3                | 2          | **5**    |
| Франкфурт      | 798.5           |8                | 2          | **10**   |
| Токио          | 244.2           | 3                | 2          | **5**    |
| Мумбаи         | 221.5           | 3                | 2          | **5**    |
| Астана         | 798.5           | 8                | 2          | **10**   |
| Сингапур       | 798.5           |  8                | 2          | **10**   |
| Найроби        | 798.5           | 8                | 2          | **10**   |
| Рио-де-Жанейро | 798.5           |  8                | 2          | **10**   |
| Сидней         | 221.5           | 3                | 2          | **5**    |

**Итого:**  
* Baseline суммарно ≈ **70** инстансов (сумма baseline по DC)  
* С учетом резервов — суммарно ≈ **92** инстанса

#### Результат L7

$$
BW_{Gbit} = RPS_{peak} \times 10{,}240\ \text{bytes} \times 8 / 10^9 \approx RPS_{peak} \times 8.192\cdot10^{-5}$$

Один L7-инстанс (NIC 10G, U=80%) → eff = 8 Гбит/с.


| Дата-центр     | RPS            | BW (Гбит/с) | L7 baseline N | L7 резерв           | L7 итого |
|----------------|---------------:|-------------------------------:|--------------:|--------------------:|---------:|
| Сан-Франциско  | 0.00           | 0.00000                         | 0             | 0                   | 0        |
| Нью-Йорк       | 0.00           | 0.00000                         | 0             | 0                   | 0        |
| Канзас-Сити  | 678.20     | 0.05556                     | 1             | 1                   | 2    |
| Лондон     | 170.10     | 0.01393                    | 1             | 1                   | 2    |
| Франкфурт      | 0.00           | 0.00000                         | 0             | 0                   | 0        |
| Токио      | 187.70     | 0.01537                   | 1             | 1                   | 2    |
| Мумбаи     | 170.10     | 0.01393                     | 1             | 1                   | 2    |
| Астана         | 0.00           | 0.00000                         | 0             | 0                   | 0        |
| Сингапур       | 0.00           | 0.00000                         | 0             | 0                   | 0        |
| Найроби        | 0.00           | 0.00000                         | 0             | 0                   | 0        |
| Рио-де-Жанейро | 0.00           | 0.00000                         | 0             | 0                   | 0        |
| Сидней     | 170.10     | 0.01393                     | 1             | 1                   | 2    |

**Итого (L7, весь мир):** 
* 5 дата-центров × 2 = 10 L7-инстанса.

## 5. Логическая схема БД

![db_diagram](pics/db_diagram.png)

Рассчитаем примерный необходимый объем памяти, для хранения каждой из сущностей:

| Сущность                                        |                                                                                                                                                                  Формула (байт/строку) |                     Количество (строк) |                                              Итоговый объём |
| ----------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | -------------------------------------: | ----------------------------------------------------------: |
| **USERS**                             | `16 (uuid - user_id) + 20 (text - username) + 40 (text - email) + 32 (bytea - password hash) + 8 (timestamp - created_at) + 8 (timestamp - updated_at)` = **124 байта / пользователь** |                    `700 000 000` (MAU) | **86.80 Гбайт** |
| **MEETINGS**                  |                                                         `16 (uuid - meeting_id) + 16 (uuid - host_id) + 60 (text - meeting_url) + 8 + 8 + 8 + 8 (timestamps)` = **124 байта / строка** |                     `16 742 770 /день` |  **3.80 ТБ**          (2.08 Гбайт / день)  |
| **PARTICIPANTS**               |                              `16 (uuid - participant_id) + 16 (uuid - user_id) + 16 (uuid - meeting_id) + 8 (timestamp - joined_at) + 8 (timestamp - left_at)` = **64 байта / строка** |                    `167 427 700 /день` |             **19.56 ТБ** (10.72 Гбайт / день) |
| **MESSAGES**                       |                                          `16 (uuid - message_id) + 16 (uuid - user_id) + 16 (uuid - meeting_id) + 150 (text - content avg) + 8 (timestamp)` = **206 байт / сообщение** |                     `83 713 850 /день` |              **31.51 ТБ**(17.25 Гбайт / день) |
| **RECORDINGS**               |                                                        `16 (uuid - recording_id) + 16 (uuid - meeting_id) + 60 (text - file_url) + 8 (timestamp - created_at)` = **100 байт / строка** |                        `227 701 /день` |                 **41.5 ГБ** (22.77 Мбайт / день) |
| **ANALYTICS**                   |                                                                       `16 (uuid - event_id) + 16 (uuid - meeting_id) + 20 (text - event_type) + 8 (timestamp)` = **60 байт / событие** |                     `83 713 850 /день` |                **9.16 ТБ** (5.02 Гбайт / день) |
| **SESSIONS**                     |                                                            `16 (uuid - user_id) + 32 (text - token) + 8 (timestamp - expires_at) + 8 (timestamp - created_at)` = **64 байта / сессия** | `600 000 000` (предположение: DAU * 2) |                     **38.40 Гбайт** |
| **ROOM_REGISTRIES**  |   `16 (uuid - room_id) + 16 (uuid - meeting_id) + 50 (text - media_host) + 4 (int - participant_count) + 10 (text - status) + 8 (timestamp - last_heartbeat)` = **104 байта / запись** |   `~627 854` (оценка concurrent rooms) |                        **65.30 Мбайт** |


### RPS / QPS
| Действие / таблица                 |  Avg W/s | Peak W/s |
| ---------------------------------- | -------: | -------: |
| Создание **MEETINGS**              |   193.78 |   581.35 |
| Join → **PARTICIPANTS** Запись     | 1 937.82 | 5 813.46 |
| Join → **PARTICIPANTS** Чтение     | 1 937.82 | 5 813.46 |
| Chat → **MESSAGES** Запись         |   968.91 | 2 906.73 |
| Запись metadata → **RECORDINGS**   |     2.64 |     7.91 |
| Analytics → **ANALYTICS**  events  |   968.91 | 2 906.73 |

### Резюме

| Таблица / сущность                       |         Оценочный объём |
| ---------------------------------------- | ----------------------: |
| **USERS**                 |         **86.80 Гбайт** |
| **MEETINGS**          |   **2.08 Гбайт / день** |
| **PARTICIPANTS**           |  **10.72 Гбайт / день** |
| **MESSAGES**             |  **17.25 Гбайт / день** |
| **RECORDINGS**       |  **22.77 Мбайт / день** |
| **ANALYTICS**           |   **5.02 Гбайт / день** |
| **SESSIONS**             |         **38.40 Гбайт** |
| **ROOM_REGISTRIES**  |         **65.30 Мбайт** |

### Описание сущностей базы данных

| Сущность | Описание |
|-----------|-----------|
| **USERS** | Информация о пользователях сервиса. Содержит технические данные (идентификатор, email, пароль) и метаданные (username, даты создания и обновления). Используется для аутентификации, идентификации и связи с другими таблицами. |
| **MEETINGS** | Данные о встречах (видеоконференциях). Содержит информацию о ведущем (`host_id`), ссылку на встречу (`meeting_url`), время начала и окончания. Используется для планирования и управления видеовстречами. |
| **PARTICIPANTS** | Таблица участников встреч. Фиксирует, какой пользователь участвовал в какой встрече, а также время подключения (`joined_at`) и выхода (`left_at`). Нужна для аналитики, подсчёта участников и истории подключений. |
| **MESSAGES** | Сообщения, отправленные участниками во время встречи. Содержит текст (`content`), отправителя (`user_id`), встречу (`meeting_id`) и время отправки (`timestamp`). Используется для отображения чата конференции и анализа коммуникаций. |
| **RECORDINGS** | Записи видеовстреч. Хранит ссылку на файл (`file_url`), дату создания (`created_at`) и связь с конкретной встречей (`meeting_id`). Используется для хранения и последующего просмотра прошедших встреч. |
| **ANALYTICS** | Таблица аналитических событий, связанных с проведением встреч. Фиксирует тип события (`event_type`, например *join*, *leave*, *message_sent*), время (`timestamp`) и встречу (`meeting_id`). Применяется для сбора метрик и анализа активности участников. |
| **SESSIONS** | Хранит активные сессии/токены пользователей: `user_id`, `token`, `expires_at`, `created_at`. Используется для быстрой проверки авторизации (lookup по токену) и инвалидации сессий.|
| **ROOM_REGISTRY** | In-memory реестр соответствий `meeting_id, media_host` `participant_count, status, last_heartbeat`. Используется control-plane/edge для маршрутизации медиапутей (чтобы ingress возвращал адрес нужного SFU).|

| Таблица           | Консистентность |
| ----------------- | --------------- | 
| **USERS**         | **strong**      |
| **MEETINGS**      | **strong**      |
| **PARTICIPANTS**  | **eventual**    |
| **MESSAGES**      | **eventual**    | 
| **RECORDINGS**    | **eventual**    | 
| **ANALYTICS**     | **eventual**    |
| **SESSIONS**      | **strong**      |
| **ROOM_REGISTRIES** | **eventual**      |


## 6. Физическая схема БД

![ph_db_diagram](pics/ph_db_diagram.jpeg)

| Сущность            | Хранилище                   | Обоснование выбора                                                                                                                                                         |
| ------------------- | --------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **USERS**           | PostgreSQL                  | Требуется строгая согласованность, транзакции и поддержка внешних ключей. Основная нагрузка — чтение профилей.              |
| **MEETINGS**        | PostgreSQL                  | Данные встреч требуют strong consistency. Диапазонное шардирование по дате позволяет архивировать старые встречи.                  |
| **RECORDINGS** | Aerospike | Метаданные о записях (ссылка на файл, идентификатор встречи) не требуют мгновенной строгой согласованности, поэтому используется eventual consistency для ускорения доступа. Основные файлы хранятся в объектном хранилище (S3).  |
| **PARTICIPANTS**    | Aerospike                   | Тысячи записей в секунду требуют низкой задержки и масштабируемости. Aerospike обеспечивает горизонтальное масштабирование и высокую доступность. |
| **MESSAGES**        | Aerospike                   | Высокая скорость вставок и чтений, масштабируемость по chat_id.                                              |
| **ANALYTICS**       | Kafka → ClickHouse | Kafka обеспечивает доставку событий, ClickHouse — быстрые запросы для дашбордов и KPI.  |
| **SESSIONS**        | Aerospike                      | Высокопроизводительное in-memory хранилище для хранения токенов с TTL. Redis Sentinel обеспечивает failover.  TODO: поменять описание на Aerospike                                    |
| **ROOM_REGISTRIES** | Aerospike                        | Контролируемый реестр для маршрутизации медиапотоков.                                          |

### Индексирование

- **PostgreSQL (USERS, MEETINGS, RECORDINGS)**
  - Primary key (по умолчанию).
  - Частые фильтры/поиски:
    - `email` (UNIQUE).
  - Составные индексы:
    - `(meeting_id, created_at)` для быстрых выборок событий по встрече и времени.

- **Aerospike (MESSAGES, PARTICIPANTS, RECORDINGS)**
  - Primary key (по умолчанию).
  - Secondary index (SI) для поиска по `timestamp`,`meeting_id`.

### Шардирование

- **PostgreSQL Citus data**
  - Вертикальное разделение: критичные транзакционные таблицы в PG.
  - Горизонтальное (при необходимости): `user_id` / `meeting_id` — хэширование.

- **Redis**
  - Redis Cluster — hash slots (16384 slots), распределяются между нодами.
  - Для сессий — consistent hashing по `user_id` или token.

- **ClickHouse**
  - Партиции по `event_date`, распределение по `meeting_id` (shard key) через `Distributed` таблицы.


### Реплицирование

- **PostgreSQL**
  - Primary → replica (streaming replication). 1 master, 4 реплики.

- **Redis**
  - Master + replicas (Redis Sentinel).

- **ClickHouse**
  - `ReplicatedMergeTree` — репликация на уровне партиций/таблиц. Click keeper. 3 ноды

- **Kafka**
  - Репликация партиций между брокерами; `replication.factor = 3`
  
- **Aerospike**
  - 11 реплик всего.

### Схема резервного копирования

- **PostgreSQL**
  - Полные бэкапы: `pg_basebackup` (ежедневно).
  - WAL-архивирование для PITR (Point-In-Time Recovery) — хранение WAL в S3.
  - Retention: ежедневные full (14d), инкрементальные + WAL (7–30d), ежемесячные cold snapshots.
  - Регулярные тестовые восстановления (smoke restore).

- **Aerospike**
  - Periodic snapshots (cold backup) + экспорт namespace в S3; обеспечивать согласованные снимки всех нод.
  - План восстановления: spin-up new cluster, restore snapshots, wait for cluster re-balance.

- **Redis**
  - RDB snapshots (каждые N минут/изменений) или AOF (если нужна долговечность).
  - Для сессий — допускается потеря кратковременных данных; комбинировать RDB + реплики.
  - Сохранять резервные копии на S3.

- **ClickHouse**
  - Backup партиций в S3 (`BACKUP` / `TABLE ... TO DISK`), реплицированные реплики упрощают восстановление.

- **Kafka**
  - репликация партиций.

- **S3 / Object storage**
  - Versioning + Lifecycle rules.

### Мультиплексирование подключений

- **PostgreSQL**
  - Использовать пуллеры: **PgBouncer** (transaction pooling).

- **Redis**
  - Клиентский пул соединений `go-redis`, reuse connections.

- **Kafka**
  - Producers/Consumers используют batching, compression и async send для throughput.

## 7. Алгоритмы

### Алгоритм распределения комнат по серверам

В системе используется гибридный алгоритм взвешенного Rendezvous hashing с учётом особенностей комнат (room-aware) для назначения комнат на SFU-серверы.

#### Учет реальной нагрузки

Алгоритм позволяет корректировать score на основе метрик сервера:

* текущее количество комнат и потоков  
* CPU / RAM utilization  
* медиапрофиль участников (bitrate, simulcast-слои)  
* сетевые задержки (RTT), packet loss

Коррекция производится через **penalty-коэффициенты**, временно понижающие вес перегруженных серверов.  

#### Минимизация ремаппинга

При добавлении новой ноды или выходе сервера из строя меняются назначения только небольшой части комнат, что минимизирует:

* количество переподключений клиентов  
* перераспределение медиапотоков  
* скачки нагрузки в кластере

Для большинства комнат выбор сервера остаётся прежним, если новая нода не даёт более высокий score.

#### Skeleton Rendezvous Hashing

Для достижения логарифмической сложности в крупных кластерах используется древовидная организация серверов (virtual skeleton).

![skeleton](pics/skeleton.jpg)

**Структура кластера:**

* Листья: физические SFU-серверы  
* Внутренние узлы: виртуальные группы, агрегирующие метрики дочерних узлов  
* Сбалансированное дерево с фактором ветвления $b$  
* Глубина дерева: $h = \log_b n$, где $n$ — количество серверов

**Алгоритм выбора сервера:**

1. Начинаем с корневого узла дерева  
2. На каждом уровне вычисляем score для всех дочерних узлов:

   $$score(\text{room}, \text{child}) = H(\text{roomID}, \text{childID}) \cdot weight(\text{child})$$
   
4. Выбираем дочерний узел с максимальным score  
5. Рекурсивно повторяем шаги 2–3 до достижения листового узла (физического сервера)

**Вычислительная сложность:**  
$O(b \times \log_b n) = O(\log n)$ операций хеширования на комнату, где $b$ — фактор ветвления (обычно 2–8).

#### Агрегация весов

* **Листовой узел (физический сервер):**  

$$weight(\text{host}) = capacity - load - penalties$$

* **Внутренний узел:**  

$$weight(\text{node}) = \sum_{child \in children} weight(child)$$

#### Формальное определение алгоритма

Базовый механизм Rendezvous hashing:

$$score(\text{room}, \text{host}) = H(\text{roomID}, \text{hostID}) \cdot weight(\text{host})$$

где $H$ — детерминированная хеш-функция (например, xxHash),  
$weight(\text{host})$ — функция доступной емкости.

Комната закрепляется за хостом с максимальным score. Для отказоустойчивости выбираются $k$ лучших серверов (primary + replicas), $k \ge 2$.

#### Характеристики алгоритма

**Преимущества:**

* **Логарифмическая сложность** — масштабируется на кластеры любого размера  
* **Детерминированность** — гарантированная консистентность при одинаковом состоянии кластера  
* **Минимальный ремаппинг** — изменения затрагивают только $O(\log n)$ путей в дереве  
* **Естественная иерархия** — отражает физическую топологию (стойки, датацентры)  
* **Балансировка нагрузки** — учет весов обеспечивает равномерное распределение

**Производительность:**

* Время выбора сервера: $O(\log n)$  
* Перебалансировка при сбое: минимальная, только для затронутых ветвей  
* Память: $O(n)$ для хранения дерева состояний

**Отказоустойчивость:**

* Автоматический обход отказавших узлов через пересчет весов  
* Возможность выбора нескольких реплик через $k$ лучших путей в дереве

## 8. Технологии

| Технология / Алгоритм                      | Область применения                                                                 | Обоснование выбора                                                                                                                                 |
| ------------------------------------------ | ---------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Go**                                     | Backend микросервисов                                                              | Высокая производительность, простота конкурентности (goroutines), богатая экосистема. Отлично подходит для высоконагруженных real-time сервисов.   |
|**gRPC**                    |Backend, взаимодействие микросервисов  |Эффективная реализация подхода Remote Procedure Call с возможностью использования ProtoBuf как быстрого и эффективного протокола для общения между микросервисами. Отлично интегрируется в Go-приложения                |
| **WebRTC**                                 | Видеоконференции / обмен медиаданными                                              | Низкая задержка, встроенная поддержка NAT traversal, шифрования и адаптации медиапотока. Прямая интеграция с SFU.                                  |
| **WebSocket**                              | Сигналинг, чат, управление состоянием                                              | Постоянное соединение для событий и уведомлений в реальном времени. Простая интеграция с браузерами и мобильными клиентами.                        |
| **SVC (Scalable Video Coding)**[[7](https://getstream.io/glossary/scalable-video-coding/)]            | Потоковое видео в многопользовательских комнатах                                   | Масштабирование видео по слоям (качество/разрешение), SFU может динамически адаптировать трафик под сеть клиента, экономия пропускной способности. |
| **Congestion Control (WebRTC GCC)**[[6](https://webrtchacks.com/probing-webrtc-bandwidth-probing-why-and-how-in-gcc/)]        | Управление пропускной способностью SFU                                             | Минимизация задержек и потерь. Адаптация битрейта под реальные условия сети для стабильного качества связи.                                        |
| **Opus DTX**[[8](https://getstream.io/resources/projects/webrtc/advanced/dtx/)]                               | Аудио в конференциях                                                               | Экономия трафика при тишине - рост масштабируемости при большом количестве участников.                                                             |
| **PostgreSQL**                             | Транзакционные данные (users, meetings)                                            | Strong consistency, поддержка сложных связей и индексов, шардирование по диапазонам (например, по дате встречи).                                   |
| **Aerospike**                                | Сессии и кэширование данных                                                        | In-memory модель с поддержкой TTL обеспечивает быстродействие при авторизации и валидации сессий.                                                  |
| **Aerospike**                              | Сообщения, участники комнат, реестры трассировки медиапотоков (SFU routing tables) | Сверхнизкие задержки и горизонтальное масштабирование при тысячах операций в секунду.                                                              |
| **Kafka**                                  | Поток событий сервиса                                                              | Асинхронное взаимодействие микросервисов, гарантированная доставка событий для аналитики.                                                          |
| **ClickHouse**                             | Аналитическое хранилище                                                            | Быстрый OLAP-движок для построения дашбордов и анализа качества связи / поведения пользователей в real-time.                                       |
| **Amazon S3**     | Хранение записей встреч                                                            | Надежность, репликация, горизонтальное масштабирование. Удобное API для загрузки/раздачи больших файлов.                                           |
| **Nginx**                                  | Обратный прокси, балансировщик                                                     | SSL-терминация, маршрутизация трафика, высокая конфигурируемость под наши протоколы (HTTPS/WebSocket).                                             |
| **Docker**                                 | Контейнеризация сервисов                                                           | Изоляция сервисов, единая среда запуска, готовность к оркестрации.                                                                                 |
| **Kubernetes**                             | Оркестрация микросервисов                                                          | Автомасштабирование SFU и backend-сервисов под нагрузку, self-healing, управление rollout-ами.                                                     |
| **VictoriaMetrics, Grafana** | Мониторинг и алертинг                                                              | Отслеживание задержек, качества медиапотока, загрузки SFU.                                                    |

## 9. Обеспечение надежности

Для обеспечения надёжности сервиса необходимо гарантировать устойчивость к сбоям на всех уровнях — аппаратном, сетевом, прикладном и организационном.

### Компоненты системы
| Компонент                                 | Способ                                   | Обоснование                                                                                                                                                                              |
| ----------------------------------------- | ---------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Nginx**                                 | **Резервирование**                       | Необходимо поддерживать пул из нескольких балансировщиков для распределения трафика и обеспечения отказоустойчивости. При падении одной ноды — трафик перенаправляется на резервную.     |
| **API Gateway**                           | **Резервирование**                       | Точка входа в систему, поэтому критично обеспечить масштабирование и дублирование. Разделение потоков web/mobile клиентов и равномерное распределение нагрузки предотвращает перегрузку. |
| **PostgreSQL**                            | **Репликация и резервное копирование**   | Используется primary–replica схема (1 master, 4 реплики) и ежедневные бэкапы с WAL-архивами. Это повышает доступность и обеспечивает восстановление при сбоях.                           |
| **Aerospike**                             | **Репликация и backup**             | Встроенная репликация (11 реплик) обеспечивает высокую доступность при больших объёмах сообщений и участников. Снимки (snapshots) хранятся в S3.                                         |
| **Aerospike** TODO: заменить описание                                | **Репликация и Sentinel**               | Master–replica + автоматический failover через Sentinel. Для хранения сессий допускается временная потеря данных, но система быстро восстанавливается.                                   |
| **ClickHouse**                            | **Репликация + Backup**              | Использование `ReplicatedMergeTree` и ClickKeeper. Cрез снапшотов старого состояния.                                                         |
| **Kafka**                                 | **Репликация брокеров**                  | Параметр `replication.factor=3` гарантирует сохранность событий даже при падении узлов.                                                                                                  |
| **S3 / Object Storage**                   | **Versioning**         | Включение версионирования и автоматическое хранение копий обеспечивает надёжность пользовательских записей.                                                                              |
| **Media-сервера (SFU)**             | **Резервирование и геораспределённость** | Несколько точек присутствия для обеспечения низкой задержки и отказоустойчивости. При падении SFU пользователи автоматически переподключаются к ближайшему узлу.                         |
| **Monitoring (VictoriaMetrics, Grafana)** | **Резервирование и алертинг**            | Метрики и логи реплицируются, что позволяет сохранять наблюдаемость даже при частичном отказе компонентов. используется реплецирование victoriaMetrics с failover                                                                               |


### Резервирование серверных ресурсов
* Использование Kubernetes для автоматического перезапуска и self-healing сервисов.
* Развёртывание всех компонентов в нескольких availability zones (multi-AZ).
* Серверы снабжаются резервными сетевыми интерфейсами, SSD-дисками, избыточной RAM и CPU.
* Horizontal Pod Autoscaler (HPA) увеличивает число экземпляров микросервисов при пиковых нагрузках.
* Liveness / Readiness probes — проверка состояния контейнеров и их перезапуск при сбоях.

### Логирование и сбор метрик
* Технические метрики: загрузка CPU, память, задержки API, состояние сетевых соединений.
* Продуктовые метрики: число участников, длительность встреч, качество медиапотока.
* Все логи и метрики отправляются в VictoriaMetrics, визуализируются в Grafana.

### Используемые архитектурные паттерны
| Паттерн                  | Назначение                                   | Применение                                                                                                                    |
| ------------------------ | -------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| **Retry**                | Повтор запросов при временных сбоях          | При недоступности SFU клиент повторно подключается к резервному серверу. Число попыток ограничено для предотвращения перегрузки. |
| **Circuit Breaker**      | Предотвращение каскадных отказов             | API Gateway и backend-сервисы временно блокируют запросы к зависимым системам при превышении порога ошибок.                          |
| **Timeouts**             | Предотвращение «зависания» запросов          | При превышении лимита времени запрос направляется к реплике или резервному сервису.                                                  |
| **Graceful Shutdown**    | Корректное завершение работы сервисов        | При остановке контейнера в Kubernetes — завершение активных соединений, освобождение ресурсов.                                       |
| **Graceful Degradation** | Снижение функциональности без полного отказа | При падении Chat-сервиса видеосвязь продолжает работать; пользователи теряют лишь часть возможностей.                                |

### Безопасность и контроль доступа
* Разграничение доступа к базам и сервисам по ролям.
* Доступ к продакшен-данным только через ограниченные сервисные аккаунты.
* Аудит действий сотрудников и логирование всех операций управления.

### Надёжность дата-центров
| Компонент              | Мера обеспечения надёжности                 | Обоснование                                                                           |
| ---------------------- | ------------------------------------------- | ------------------------------------------------------------------------------------- |
| **Дата-центры**   | **Геораспределённое резервирование**        | Размещение узлов в разных регионах для защиты от аварий и катастроф.                  |
| **Электропитание**     | **Двойные линии + UPS + дизель-генераторы** | При потере внешнего питания — автоматическое переключение на резервные источники.     |
| **Системы охлаждения** | **Дублирование контуров**                   | Предотвращает перегрев и отказ оборудования при сбое охлаждения.                      |
| **Сети**               | **Резервные каналы связи и маршруты**       | Снижение риска потери связи при повреждении магистралей.                              |
| **Учения персонала**   | **Регулярные аварийные тренировки**         | Отработка сценариев отключения DC, симуляция потерь, контроль времени восстановления. |

## 10. Схема проекта

### Общая схема

![service_diagram](pics/zoom_service_diagram.png)

*Здесь и далее приняты следующие обозначения:*

![symbols](pics/symbols.png)

### 1. Пользователи

#### Групповые чаты
В группах:
- **WSH** знает только пользователей, не группы;
- Сообщение идёт в **Message Service**, оттуда в **Kafka**;
- **Group Message Handler** читает Kafka, получает участников из **Group Service**, и отправляет сообщение каждому через его WSH.

**Group Service** хранит данные о группах в **Postgres** и кэширует в **Aerospike**.

### 2. Видеозвонки

#### Ключевые компоненты
- **Ingress Gateway** — публичная точка входа, принимает signaling и медиапотоки от клиентов, маршрутизирует к внутренним сервисам.  
- **STUN** — сервер для определения внешнего IP/порта клиента (NAT traversal). Он нужен, чтобы стороны могли установить прямое соединение или подключиться к медиахосту.  
- **Room Registry** — хранит метаданные комнат и участников, работает поверх **Aerospike** для быстрого доступа и распределённой консистентности.  
- **Media Hosts (SFU)** — селективные форвардеры потоков, принимают RTP от клиентов и рассылают остальным участникам.  
- **Logger + File Creator** — сохраняют записи звонков: Logger пишет куски, File Creator собирает и сохраняет итоговый файл в **S3**.  
- **Health Monitor** — следит за состоянием медиахостов и балансирует нагрузку.

#### Установка соединения (signaling)
1. **Client1 → Ingress Gateway → Signaling Service:** запрос на звонок к Client2.  
2. **Signaling Service** проверяет через **User Service**, можно ли инициировать звонок, затем создаёт/ищет комнату в **Room Registry**.  
3. **Signaling Service** уведомляет **Ingress** у **Client2**, спрашивает, принимает ли звонок.  
4. После принятия — обмен ICE-кандидатами, включая публичные адреса, полученные через **STUN**.  
5. Стороны начинают передачу потоков RTP через **Ingress Gateway** на **Media Host (SFU)**.

#### SFU (Media Hosts) — основной медиапоток
- Каждый участник отправляет свой медиапоток на **Media Host**.  
- SFU передаёт эти потоки другим участникам, выбирая качество и поток в зависимости от сети.  
- При необходимости SFU может понижать разрешение или фреймрейт.  
- Все настройки адаптации выполняются динамически, без пересоздания соединения.

#### NAT traversal и STUN
**STUN** используется клиентами, чтобы:
- определить свой внешний IP/порт;
- сообщить эти данные через signaling для ICE;
- тем самым обеспечить прямую маршрутизацию медиапотоков к SFU.  

STUN обязателен для всех участников, независимо от типа сети.

### 3. Обмен сообщениями

#### Общая идея  
Когда пользователь **Client1** отправляет сообщение пользователю **Client2**, оба подключены к системе через **WebSocket-обработчики (WSH)** — лёгкие серверы, поддерживающие постоянное соединение с активными пользователями.  
Таких серверов много, они распределены по регионам для уменьшения задержек.

#### WebSocket-обработчики и менеджер
Каждый WSH может держать до ~60 000 соединений (ограничено количеством портов).  
Для управления используется **WebSocket-менеджер**, который хранит в **Aerospike**:
- кто из пользователей подключён к какому обработчику;
- какие пользователи подключены к конкретному обработчику.

Если пользователь теряет соединение и подключается к другому серверу, менеджер обновляет данные в Aerospike.

#### Message Service
**Сервис сообщений** хранит все сообщения и их статусы.  
Он работает поверх **Aerospike**, так как она масштабируется горизонтально и подходит для постоянно растущего объёма данных и фиксированного набора запросов (по id пользователя, статусу, id сообщения и т. д.).

#### Процесс доставки сообщения (оба онлайн)
1. **Client1 → WSH1:** отправка сообщения.  
2. **WSH1** спрашивает у менеджера, где подключён **Client2** → узнаёт, что на **WSH2**.  
3. **WSH1** сохраняет сообщение через **Message Service**, получает `message_id`.  
4. **WSH1 → WSH2:** передача сообщения.  
5. **WSH2** доставляет **Client2**, обновляет статус (“доставлено”, “прочитано”) в Aerospike.  
6. **WSH2** сообщает об этом обратно **WSH1**, чтобы уведомить **Client1**.

Чтобы сократить обращения к менеджеру, каждый WSH кэширует:
- пользователей, подключённых к нему самому;
- недавние связи “пользователь ↔ обработчик”.  
Кэш живёт недолго, потому что подключения часто меняются.

#### Когда получатель офлайн
Если **Client3** офлайн:
- **WSH1** сохраняет сообщение в Message Service;  
- при подключении **Client3 → WSH3**, тот запрашивает все недоставленные сообщения и отправляет их.  
После доставки статус обновляется и передаётся обратно.

#### Race Condition
Если **Client3** подключился прямо во время отправки, может произойти гонка состояний.  
Решение: периодический **polling** — обработчики регулярно опрашивают Message Service.

#### Локальный кэш устройства
Если пользователь был офлайн, сообщение временно сохраняется в локальной БД устройства и отправляется при восстановлении соединения.

## Источники
1. https://www.demandsage.com/zoom-statistics/
2. https://www.marketingscoop.com/blog/zoom-monthly-active-users-the-ultimate-guide-2024/
3. https://www.statista.com/statistics/1259936/distribution-of-zoomus-traffic-by-country/
4. https://www.zoom.com/en/blog/how-you-zoomed-over-the-past-year-2021/?lang=en-US
5. https://support.zoom.com/hc/ru/article?id=zm_kb&sysparm_article=KB0060759
6. https://webrtchacks.com/probing-webrtc-bandwidth-probing-why-and-how-in-gcc/
7. https://getstream.io/glossary/scalable-video-coding/
8. https://getstream.io/resources/projects/webrtc/advanced/dtx/
